{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautifulsoup Handbook : Data Collection for Ai through Webscraping\n",
    "### *Ashraful Islam Mahi*\n",
    "*Full Stack Python Developer | ML & Robotics Enthusiast | Founder & CEO, PytronLab*\n",
    "\n",
    "![Soup Logo](bsoup.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Role of Data Collection in Data Science & Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is data collection?\n",
    "- Data collection includes gathering, processing, and storing data from various sources to train machine learning models. It includes structured (databases, spreadsheets) and unstructured (images, text, audio) data collected from sensors, social media, APIs, surveys, and more.\n",
    "\n",
    "#### Imporatnce of data collection:\n",
    "1. Model Accuracy – High-quality data ensures better predictions and decision-making.\n",
    "2. Training Efficiency – More diverse and relevant data improve learning speed and performance.\n",
    "3. Bias Reduction – Proper data collection minimizes biases and enhances fairness in AI models.\n",
    "4. Real-world Adaptation – AI models need continuous data to stay relevant and perform well in dynamic environments.\n",
    "5. Automation & Insights – Well-collected data helps automate processes and extract meaningful insights for  businesses and research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is webscraping? \n",
    "- Web scraping is the automated process of extracting data from websites. It involves using scripts or tools to access web pages, retrieve information, and store it in a structured format for analysis.\n",
    "\n",
    "- How It Works:\n",
    "\n",
    "![Scraper](scraper.png)\n",
    "\n",
    "  1. Sending Requests – A scraper sends a request to a webpage using libraries like requests in Python.\n",
    "  2. Parsing HTML – Extracting relevant data using tools like BeautifulSoup or Scrapy.\n",
    "  3. Storing Data – Saving the extracted information in a CSV, database, or other formats for further use.\n",
    "- Common Uses:\n",
    "  - Price comparison\n",
    "  - Market research\n",
    "  - Sentiment analysis\n",
    "  - Lead generation\n",
    "  - AI training data collection\n",
    "\n",
    "#### Popular webscraping frameworks of pythons & their use cases: \n",
    "\n",
    "| Frameworks | When to Use |\n",
    "|------------|-------------|\n",
    "| 1. Beautifulsoup & Requests | Static website with small to medium project|\n",
    "| 2. Scrapy | Static website but Large Project |\n",
    "| 3. Selenium | Dynamic website with direct interaction requirement |\n",
    "| 4. Scrapy-Playwright | Dynamic website with large project |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Beautifulsoup:\n",
    "\n",
    "#### What is Beautifulsoup?\n",
    "- Beautiful Soup is a Python package for parsing HTML and XML documents, including those with malformed markup. It creates a parse tree for documents that can be used to extract data from HTML, which is useful for web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Installing Beautifulsoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4\n",
    "%pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Importing Beautifulsoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Best Practices of using Beautifulsoup:\n",
    "   - Don't send separate requests for each extraction. \n",
    "   - Extract the html of the web page at once using single request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's scrape a website [Quotes to Scrape](https://quotes.toscrape.com/)\n",
    "#### *Technique:* First HTML retrive then Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-1:** Get the url of the web page you want to scrape: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://quotes.toscrape.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-2:** Fetch the HTML code of that page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "respose = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-3:** Check the status code of the 'response'. \n",
    " - If respose.status_code == 200 : HTML has successfully fetched.\n",
    " - If respose.status_code != 200 : HTML hasn't fetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respose.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-4:** Save the HTML code in a HTML file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_file_name = 'quote.html'\n",
    "\n",
    "with open(html_file_name,'w', encoding='utf8') as file:\n",
    "  file.write(respose.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-5:** Create Soup object using beautiful soup to scrape the html:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_file_name = 'quote.html'\n",
    "\n",
    "with open(html_file_name,'r',encoding='utf8') as file:\n",
    "  html_content = file.read()\n",
    "  soup = bs(html_content)\n",
    "  print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping using the soup object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Find 'Page Title':** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quotes to Scrape\n"
     ]
    }
   ],
   "source": [
    "title = soup.find('title')\n",
    "\n",
    "print(title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Select first quote according to class name:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n"
     ]
    }
   ],
   "source": [
    "first_quote = soup.find('span',class_='text')\n",
    "\n",
    "print(first_quote.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Select all quotes according to class name:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of quotes: 10\n",
      "\n",
      "Quotes:\n",
      "------\n",
      "1.“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "2.“It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
      "3.“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "4.“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "5.“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
      "6.“Try not to become a man of success. Rather become a man of value.”\n",
      "7.“It is better to be hated for what you are than to be loved for what you are not.”\n",
      "8.“I have not failed. I've just found 10,000 ways that won't work.”\n",
      "9.“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
      "10.“A day without sunshine is like, you know, night.”\n"
     ]
    }
   ],
   "source": [
    "quotes = soup.find_all('span',class_='text')\n",
    "print(f\"No of quotes: {len(quotes)}\\n\")\n",
    "\n",
    "print(\"Quotes:\\n------\")\n",
    "for i in range(len(quotes)):\n",
    "  print(f\"{i+1}.{quotes[i].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Find the text of quote using attribute:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n"
     ]
    }
   ],
   "source": [
    "quote_text = soup.find('span',attrs={'itemprop':'text'}).text\n",
    "\n",
    "print(quote_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Select \"top 10 tags\" using the text inside it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>Top Ten tags</h2>\n"
     ]
    }
   ],
   "source": [
    "tags_title = soup.find('h2',string='Top Ten tags')\n",
    "print(tags_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation using soup object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Find parent of 'tags_title':**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_box = tags_title.parent\n",
    "\n",
    "print(tag_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Find the next sibling of \"tags_title':**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"tag-item\">\n",
      "<a class=\"tag\" href=\"/tag/love/\" style=\"font-size: 28px\">love</a>\n",
      "</span>\n"
     ]
    }
   ],
   "source": [
    "first_tag_span = tags_title.find_next_sibling()\n",
    "\n",
    "print(first_tag_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Find the previous sibling of \"tags_title':**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>Top Ten tags</h2>\n"
     ]
    }
   ],
   "source": [
    "h2_again = first_tag_span.find_previous_sibling()\n",
    "\n",
    "print(h2_again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. First children of tag box:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>Top Ten tags</h2>\n"
     ]
    }
   ],
   "source": [
    "tag_children = soup.find('div',class_ = 'tags-box').children\n",
    "tag_box_children = list(tag_children)\n",
    "final_children = [x for x in tag_box_children if x != '\\n']\n",
    "\n",
    "top_h2_tag = final_children[0]\n",
    "\n",
    "print(top_h2_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Get the href value of top_tag:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tag/love/\n"
     ]
    }
   ],
   "source": [
    "first_tag_span_href = first_tag_span.a['href']\n",
    "\n",
    "print(first_tag_span_href)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Extract any attribute with this method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_quote['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'font-size: 28px'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_tag_span.a['style']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's scrape another website [Book to Scrape](https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-1:** Extract 1st book info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-2:** Extract the html of that book page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-3:** Check the status code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-4:** Write the html code into a html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'book.html'\n",
    "\n",
    "with open(file_name,'w',encoding='utf8') as file:\n",
    "  file.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-5:** Read the html file and turn that into a soup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'book.html'\n",
    "\n",
    "with open(file_name,'r',encoding='utf8') as file:\n",
    "  content = file.read()\n",
    "  soup = bs(content)\n",
    "  \n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book Name: A Light in the Attic\n"
     ]
    }
   ],
   "source": [
    "# book name extraction\n",
    "book_name = soup.find('div',class_ = 'product_main').h1\n",
    "print(f\"Book Name: {book_name.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book Catagory:\n",
      "Poetry\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# catagory name extraction\n",
    "items = soup.find('ul',class_ = 'breadcrumb').children\n",
    "list_items = list(items)\n",
    "filtered_list = [x for x in list_items if x != '\\n']\n",
    "catagory = filtered_list[2]\n",
    "print(f\"Book Catagory:{catagory.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings: Three\n"
     ]
    }
   ],
   "source": [
    "# rating extraction\n",
    "all_p = soup.find('div',class_ = 'product_main')\n",
    "p_items = all_p.find_all('p')\n",
    "rating = p_items[2]['class']\n",
    "\n",
    "print(f\"Ratings: {rating[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Info:\n",
      "-----------\n",
      "UPC: a897fe39b1053632\n",
      "Product Type: Books\n",
      "Price (excl. tax): Â£51.77\n",
      "Price (incl. tax): Â£51.77\n",
      "Tax: Â£0.00\n",
      "Availability: In stock (22 available)\n",
      "Number of reviews: 0\n"
     ]
    }
   ],
   "source": [
    "# product info extraction\n",
    "left_info = soup.find_all('th')\n",
    "right_info = soup.find_all('td')\n",
    "\n",
    "print(\"Product Info:\")\n",
    "print(\"-----------\")\n",
    "for left,right in zip(left_info,right_info):\n",
    "  print(f\"{left.text}: {right.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image link: https://books.toscrape.com/../../media/cache/fe/72/fe72f0532301ec28892ae79a629a293c.jpg\n"
     ]
    }
   ],
   "source": [
    "# Image Extraction\n",
    "\n",
    "image_div = soup.find('div',class_ = 'item active').img['src']\n",
    "\n",
    "print(f'Image link: https://books.toscrape.com/{image_div}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://books.toscrape.com/index.html'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'books.html'\n",
    "\n",
    "with open(file_name,'w',encoding='utf8') as file:\n",
    "  file.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'books.html'\n",
    "\n",
    "with open(file_name,'r',encoding='utf8') as file:\n",
    "  content = file.read()\n",
    "  \n",
    "  soup = bs(content)\n",
    "  print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "#Detect page number\n",
    "\n",
    "list = soup.find('li',class_ = 'current').text\n",
    "no_of_page = int(list.strip().split(' ')[-1])\n",
    "print(no_of_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop throuh all pages , if the page number is known\n",
    "for page in range(1,no_of_page+1):\n",
    "  print(f\"Page no----> {page}\")\n",
    "  page_url = f'https://books.toscrape.com/catalogue/page-{page}.html'\n",
    "  response = requests.get(page_url)\n",
    "  content = response.text\n",
    "  soup = bs(content)\n",
    "  books = soup.find_all('article',class_ = 'product_pod')\n",
    "  print(f\"No of book: {len(books)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bulk Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************************\n",
      "Page no----> 1\n",
      "*************************\n",
      "\n",
      "\n",
      "*************************\n",
      "Page no----> 2\n",
      "*************************\n",
      "\n",
      "\n",
      "*************************\n",
      "Page no----> 3\n",
      "*************************\n",
      "\n",
      "\n",
      "*************************\n",
      "Page no----> 4\n",
      "*************************\n",
      "\n",
      "\n",
      "Scraping Done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs \n",
    "import builtins\n",
    "import pandas as pd\n",
    "\n",
    "file_name = 'image_link.txt'\n",
    "# with open(file_name,'w') as file:\n",
    "#     pass\n",
    "\n",
    "\n",
    "\n",
    "book_dict = {\n",
    "  'name': [],\n",
    "  'price': [],\n",
    "  'catagory': [],\n",
    "  'ratings': [],\n",
    "  'upc': [],\n",
    "  'availability': [],\n",
    "  'in_stock': [],\n",
    "  'image_link': []\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                  'Chrome/134.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "page = 1\n",
    "while True:\n",
    "    print(\"\\n*************************\")\n",
    "    print(f\"Page no----> {page}\")\n",
    "    print(\"*************************\\n\")\n",
    "    \n",
    "    page_url = f'https://books.toscrape.com/catalogue/page-{page}.html'\n",
    "    response = requests.get(page_url, headers=headers)\n",
    "    content = response.text\n",
    "    # Use a separate variable for the listing page soup\n",
    "    listing_soup = bs(content, 'html.parser')\n",
    "    books = listing_soup.find_all('article', class_='product_pod')\n",
    "    \n",
    "    for book in books:\n",
    "        # Fix quotes in the f-string below\n",
    "        book_href = book.find(\"a\")[\"href\"]\n",
    "        book_url = f'https://books.toscrape.com/catalogue/{book_href}'\n",
    "        response = requests.get(book_url, headers=headers)\n",
    "        response.encoding = 'utf-8'\n",
    "        book_soup = bs(response.text, 'html.parser')\n",
    "        \n",
    "        # Book name extraction\n",
    "        book_name_tag = book_soup.find('div', class_='product_main').h1\n",
    "        book_name = book_name_tag.text if book_name_tag else \"No Title\"\n",
    "        book_dict['name'].append(book_name)\n",
    "        \n",
    "        # Price extraction\n",
    "        price_tag = book_soup.find('div', class_='product_main').p\n",
    "        price = price_tag.text if price_tag else \"No Price\"\n",
    "        book_dict['price'].append(price)\n",
    "         \n",
    "        # Catagory extraction\n",
    "        items = book_soup.find('ul', class_='breadcrumb').children\n",
    "        list_items = builtins.list(items)\n",
    "        filtered_list = [x for x in list_items if x != '\\n']\n",
    "        catagory = filtered_list[2].get_text(strip=True) if len(filtered_list) >= 3 else \"Not Available\"\n",
    "        book_dict['catagory'].append(catagory)\n",
    "        \n",
    "        # Rating extraction: assumes the third <p> tag in product_main holds the rating\n",
    "        product_main = book_soup.find('div', class_='product_main')\n",
    "        p_items = product_main.find_all('p')\n",
    "        rating = p_items[2]['class'][1] if len(p_items) >= 3 and len(p_items[2].get(\"class\", [])) > 1 else \"Not Available\"\n",
    "        book_dict['ratings'].append(rating)\n",
    "        \n",
    "        # UPC extraction\n",
    "        upc_th = book_soup.find('th', string='UPC')\n",
    "        upc = upc_th.find_next_sibling().text if upc_th else \"Not Available\"\n",
    "        book_dict['upc'].append(upc)\n",
    "        \n",
    "        # Availability extraction\n",
    "        availability_th = book_soup.find('th', string='Availability')\n",
    "        availability = availability_th.find_next_sibling().text if availability_th else \"Not Available\"\n",
    "        book_dict['availability'].append(availability)\n",
    "        \n",
    "        # Instock extraction\n",
    "        if '(' in availability:\n",
    "            instock = availability.split('(')[1].split(' ')[0]\n",
    "        else:\n",
    "            instock = \"Not Available\"\n",
    "        book_dict['in_stock'].append(instock)\n",
    "        \n",
    "        # Image extraction and convert relative URL to absolute URL\n",
    "        image_tag = book_soup.find('div', class_='item active').img\n",
    "        if image_tag:\n",
    "            image_src = image_tag.get('src', '')\n",
    "            image_src = image_src.replace('../', '')\n",
    "            image_link = f'https://books.toscrape.com/{image_src}'\n",
    "        else:\n",
    "            image_link = \"Not Available\"\n",
    "        book_dict['image_link'].append(image_link)\n",
    "        with open(file_name,'a',encoding='utf8') as file:\n",
    "            file.write(image_link+'\\n')\n",
    "  \n",
    "    # Use listing_soup to check for the next button\n",
    "    # next_button = listing_soup.find('li', class_='next')\n",
    "    if page > 3:\n",
    "        print(\"\\nScraping Done\")\n",
    "        break\n",
    "    else:\n",
    "        page += 1\n",
    "\n",
    "df = pd.DataFrame(book_dict)\n",
    "df.to_excel('book.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download Images:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir('book_images')\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                  'Chrome/134.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "with open('image_link.txt', 'r') as f:\n",
    "  links_txt = f.read()\n",
    "  links_list = links_txt.split('\\n')\n",
    "\n",
    "for i ,image_url in enumerate(links_list):\n",
    "  response = requests.get(image_url,headers=headers)\n",
    "  with open(f'book_images/{i+i}.jpg', 'wb') as imagefile:\n",
    "    imagefile.write(response.content)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bsoup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
